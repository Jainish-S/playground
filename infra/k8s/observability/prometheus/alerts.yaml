# Prometheus Alerting Rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: observability
data:
  alerts.yaml: |
    groups:
      - name: node-alerts
        rules:
          - alert: NodeDown
            expr: up{job="kubernetes-nodes"} == 0
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Node {{ $labels.instance }} is down"

          - alert: NodeHighCPU
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High CPU on {{ $labels.instance }}"

          - alert: NodeHighMemory
            expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 85
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "High memory on {{ $labels.instance }}"

          - alert: NodeDiskFull
            expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 85
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "Disk nearly full on {{ $labels.instance }}"

      - name: pod-alerts
        rules:
          - alert: PodCrashLooping
            expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} is crash looping"

          - alert: PodNotReady
            expr: kube_pod_status_ready{condition="true"} == 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: "Pod {{ $labels.pod }} is not ready"

      - name: pvc-alerts
        rules:
          - alert: PVCFull
            expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "PVC {{ $labels.persistentvolumeclaim }} is > 85% full"

      - name: hpa_alerts
        interval: 30s
        rules:
          # HPA cannot scale due to quota
          - alert: HPAConstrainedByQuota
            expr: |
              kube_horizontalpodautoscaler_status_desired_replicas >
              kube_horizontalpodautoscaler_status_current_replicas
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "HPA {{ $labels.horizontalpodautoscaler }} constrained"
              description: "Desired {{ $value }} replicas but stuck at current. Check resource quotas and pod limits."

          # Latency SLA breach despite max scaling
          - alert: LatencySLABreachAtMaxScale
            expr: |
              histogram_quantile(0.99,
                rate(guardrail_request_latency_seconds_bucket[2m])
              ) > 0.1
              AND
              kube_deployment_status_replicas{deployment="guardrail-server", namespace="guardrails-platform"} >= 8
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: "P99 latency > 100ms despite scaling to {{ $value }} pods"
              description: "System at capacity. Consider increasing resource limits or reducing traffic."

          # HPA disabled or misconfigured
          - alert: HPANotScaling
            expr: |
              kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false", namespace="guardrails-platform"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "HPA {{ $labels.horizontalpodautoscaler }} is not scaling"
              description: "Check HPA configuration and ensure custom metrics are available via Prometheus Adapter."
